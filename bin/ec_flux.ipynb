{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kurtosis, skew\n",
    "from itertools import groupby\n",
    "from setting import *\n",
    "\n",
    "def quality_control(unit_k, df, flags):\n",
    "    \"\"\"Quality control for eddy covariance flux data\n",
    "    Vickers, D., and L. Mahrt, 1997: Quality control and flux sampling problems for tower and aircraft data.\n",
    "    J. Atmos. Oceanic tech., 14, 512-526\n",
    "    \"\"\"\n",
    "    instrument(df, flags)\n",
    "\n",
    "    print('%-32s' % '  Diagnostics', end='')\n",
    "    for var in ['Ux', 'Uy', 'Uz', 'Ts', 'co2', 'h2o']: print('%20s' % var, end='')\n",
    "    print()\n",
    "\n",
    "    print('%-32s' % '  Spike fraction', end='')\n",
    "    for var in ['Ux', 'Uy', 'Uz', 'Ts', 'co2', 'h2o']:\n",
    "        spikes(df, var, flags)\n",
    "    print('')\n",
    "\n",
    "    print('%-32s' % '  Empty bin ratio', end='')\n",
    "    for var in ['Ux', 'Uy', 'Uz', 'Ts', 'co2', 'h2o']: amplitude_resolution(df, var, flags)\n",
    "    print()\n",
    "\n",
    "    print('%-32s' % '  Dropouts, extreme dropouts', end='')\n",
    "    for var in ['Ux', 'Uy', 'Uz', 'Ts', 'co2', 'h2o']: dropouts(df, var, flags)\n",
    "    print()\n",
    "\n",
    "    print('%-32s' % '  Range', end='')\n",
    "    absolute_limits(df, flags)\n",
    "    print()\n",
    "\n",
    "    print('%-32s' % '  Skewness, kurtosis', end='')\n",
    "    for var in ['Ux', 'Uy', 'Uz', 'Ts', 'co2', 'h2o']:\n",
    "        detrend(df, var)\n",
    "        higher_moment_statistics(df, var, flags)\n",
    "    print()\n",
    "\n",
    "    print('%-32s' % '  Normalized Harr mean, variance', end='')\n",
    "    for var in ['Ux', 'Uy', 'Uz', 'Ts', 'co2', 'h2o']: discontinuities(df, var, flags)\n",
    "    print('\\n')\n",
    "\n",
    "    nonstationary(df, unit_k, flags)\n",
    "    print()\n",
    "\n",
    "    first = True\n",
    "    if any(flags.values()):\n",
    "        print('  Flags: ', end='')\n",
    "        for flag in flags:\n",
    "            if flags[flag] == 1:\n",
    "                print(f'{flag}' if first else f', {flag}', end='')\n",
    "                first = False\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def instrument(df, flags):\n",
    "    \"\"\"Instrument diagnostics\n",
    "    If available records for current time period is less than a threshold, or is more than 18000, a flag is placed\n",
    "    \"\"\"\n",
    "\n",
    "    ratio = float(len(df[df['diag'] < 16])) / (AVERAGING_PERIOD_MINUTES * SECONDS_IN_MINUTE * FREQUENCY_HZ)\n",
    "    print(f'  Available data fraction: {ratio:.3f}\\n')\n",
    "    flags['instrument'] = 1 if (ratio < QC_THRESHOLDS['instrument'] or ratio > 1.0) else 0\n",
    "\n",
    "\n",
    "def spikes(df, var, flags):\n",
    "    \"\"\"Detect and remove spikes\n",
    "    The method computes the mean and standard deviation for a series of moving windows of length L1. The window moves\n",
    "    one point at a time through the series. Any point in the window that is more than 3.5 standard deviations from the\n",
    "    window mean is considered a spike. The point is replaced using linear interpolation between data points. When four\n",
    "    or more consecutive points are detected, they are not considered spikes and are not replaced. The entire process is\n",
    "    repeated until no more spikes are detected. During the second pass, when the standard deviations may be smaller if\n",
    "    spikes were replaced on the previous pass, the threshold for spike detection increases to 3.6 standard deviations\n",
    "    and a like amount for each subsequent pass. The record is hard flagged when the total number of spikes replaced\n",
    "    exceeds 1% of the total number of data points.\n",
    "    \"\"\"\n",
    "    CONSECUTIVE_SPIKES = 3\n",
    "    L1_SECONDS = 300\n",
    "    std_threshold = 3.5\n",
    "    window_position = 0\n",
    "    spike_ind = []\n",
    "    n_spikes = 0\n",
    "\n",
    "    L1 = min(int(L1_SECONDS * FREQUENCY_HZ), len(df))\n",
    "\n",
    "    while True:\n",
    "        while window_position + L1 <= len(df):\n",
    "            window_mean = np.nanmean(df.loc[window_position:window_position + L1, var])\n",
    "            window_std = np.nanstd(df.loc[window_position:window_position + L1, var])\n",
    "\n",
    "            k0 = window_position + 1\n",
    "            while k0 < L1:\n",
    "                if abs(df.loc[k0, var] - window_mean) > std_threshold * window_std:\n",
    "                    for k in range(k0 + 1, L1):\n",
    "                        if abs(df.loc[k, var] - window_mean) < std_threshold * window_std: break\n",
    "\n",
    "                    if (df.loc[k, 'TmStamp'] - df.loc[k0, 'TmStamp']).total_seconds() > CONSECUTIVE_SPIKES / FREQUENCY_HZ:\n",
    "                        # When four or more consecutive points are detected, they are not considered spikes\n",
    "                        pass\n",
    "                    else:\n",
    "                        # Replace spikes using linear interpolation between data points\n",
    "                        for kk in range(k0, k):\n",
    "                            n_spikes += 1\n",
    "                            if kk not in spike_ind: spike_ind.append(kk)\n",
    "                            df.loc[kk, var] = np.interp(\n",
    "                                (df.loc[kk, 'TmStamp'] - df.loc[k0 - 1, 'TmStamp']).total_seconds(),\n",
    "                                [0, (df.loc[k, 'TmStamp'] - df.loc[k0 - 1, 'TmStamp']).total_seconds()],\n",
    "                                [df.loc[k0 - 1, var], df.loc[k, var]]\n",
    "                            )\n",
    "                    k0 = k\n",
    "                else:\n",
    "                    k0 += 1\n",
    "\n",
    "            window_position += 1\n",
    "\n",
    "        spike_ratio = float(len(spike_ind)) / float(len(df))\n",
    "        flags[f'{var}_spikes'] = 1 if (spike_ratio > QC_THRESHOLDS['spike']) else 0\n",
    "\n",
    "        if n_spikes == 0:\n",
    "            print('%20.3f' % (spike_ratio), end='')\n",
    "            return\n",
    "        else:\n",
    "            window_position = 0\n",
    "            n_spikes = 0\n",
    "            std_threshold += 0.1\n",
    "\n",
    "\n",
    "def amplitude_resolution(df, var, flags):\n",
    "    \"\"\"Detect resolution problems\n",
    "    A problem is detected by computing a series of discrete frequency distributions for half-overlapping windows of\n",
    "    length 1000 data points. These windows move one-half the window width at a time through the series. For each window\n",
    "    position, the number of bins is set to 100 and the interval for the distribution is taken as the smaller of seven\n",
    "    standard deviations and the range. When the number of empty bins in the discrete frequency distribution exceeds a\n",
    "    critical threshold value, the record is hard flagged as a resolution problem.\n",
    "    \"\"\"\n",
    "\n",
    "    L1 = 1000\n",
    "    N_BINS = 100.0\n",
    "\n",
    "    L1 = min(L1, len(df))\n",
    "    empty_bins = 0\n",
    "    window_position = 0\n",
    "\n",
    "    while (window_position + L1 <= len(df)):\n",
    "        window_mean = np.nanmean(df.loc[window_position:window_position + L1, var])\n",
    "        window_std = np.nanstd(df.loc[window_position:window_position + L1, var])\n",
    "        distribution = min(7 * window_std, np.ptp(df.loc[window_position:window_position + L1, var]))\n",
    "        bin_edges = np.arange(\n",
    "            window_mean - distribution / 2.0,\n",
    "            window_mean + distribution / 2.0 + distribution / N_BINS,\n",
    "            distribution / N_BINS,\n",
    "        )\n",
    "\n",
    "        hist, _ = np.histogram(df.loc[window_position:window_position + L1, var], bins=bin_edges)\n",
    "        empty_bins = max(empty_bins, float(np.count_nonzero(hist==0)) / N_BINS)\n",
    "\n",
    "        window_position += 250\n",
    "\n",
    "    print('%20.3f' %(empty_bins), end='')\n",
    "    flags[f'{var}_resolution'] = 1 if (empty_bins > QC_THRESHOLDS['empty_bins'])  else 0\n",
    "\n",
    "\n",
    "def dropouts(df, var, flags):\n",
    "    \"\"\"Detect dropouts\n",
    "    Dropouts are identified using the same window and frequency distributions used for the resolution problem.\n",
    "    Consecutive points that fall into the same bin of the frequency distribution are tentatively identified as dropouts.\n",
    "    When the total number of dropouts in the record exceeds a threshold value, the record is flagged for dropouts.\n",
    "    \"\"\"\n",
    "    L1 = 1000\n",
    "    N_BINS = 100.0\n",
    "\n",
    "    L1 = min(L1, len(df))\n",
    "    dropout_ratio = 0\n",
    "    extreme_dropout_ratio = 0\n",
    "    window_position = 0\n",
    "\n",
    "    while (window_position + L1 <= len(df)):\n",
    "        window_mean = np.nanmean(df.loc[window_position:window_position + L1, var])\n",
    "        window_std = np.nanstd(df.loc[window_position:window_position + L1, var])\n",
    "        distribution = min(7 * window_std, np.ptp(df.loc[window_position:window_position + L1, var]))\n",
    "        bin_edges = np.arange(\n",
    "            window_mean - distribution / 2.0,\n",
    "            window_mean + distribution / 2.0 + distribution / N_BINS,\n",
    "            distribution / N_BINS,\n",
    "        )\n",
    "\n",
    "        #https://stackoverflow.com/questions/6352425/whats-the-most-pythonic-way-to-identify-consecutive-duplicates-in-a-list\n",
    "        bins = np.digitize(df.loc[window_position:window_position + L1, var], bin_edges)\n",
    "        grouped_bins = [(k, sum(1 for i in g)) for k, g in groupby(bins)]\n",
    "\n",
    "        max_dropouts = max(grouped_bins, key=lambda x: x[1])\n",
    "\n",
    "        dropout_ratio = max(dropout_ratio, float(max_dropouts[1]) / float(len(df)))\n",
    "        if  (max_dropouts[0] < 10 or max_dropouts[0] > 90):\n",
    "            extreme_dropout_ratio = max(extreme_dropout_ratio, float(max_dropouts[1]) / float(len(df)))\n",
    "\n",
    "        window_position += 250\n",
    "\n",
    "    print('%20s' %(f'{dropout_ratio:.3f}, {extreme_dropout_ratio:.3f}'), end='')\n",
    "    flags[f'{var}_dropouts'] = 1 if (dropout_ratio > QC_THRESHOLDS['dropouts']) else 0\n",
    "    flags[f'{var}_extreme_dropouts'] = 1 if (extreme_dropout_ratio > QC_THRESHOLDS['extreme_dropouts']) else 0\n",
    "\n",
    "\n",
    "def absolute_limits(df, flags):\n",
    "    \"\"\"Detect unrealistic data out of their physical ranges\n",
    "    Unrealistic data are detected and hard flagged by simply comparing the minimum and maximum value of all points in\n",
    "    the record to some fixed limits considered unphysical.\n",
    "    \"\"\"\n",
    "    print('%20s' %(f'[{df[\"Ux\"].min():.3f}, {df[\"Ux\"].max():.3f}]'), end='')\n",
    "    flags['Ux_absolute_limits'] = 1 if any(abs(df['Ux']) > 30.0) else 0\n",
    "\n",
    "    print('%20s' %(f'[{df[\"Uy\"].min():.3f}, {df[\"Uy\"].max():.3f}]'), end='')\n",
    "    flags['Uy_absolute_limits'] = 1 if any(abs(df['Uy']) > 30.0) else 0\n",
    "\n",
    "    print('%20s' %(f'[{df[\"Uz\"].min():.3f}, {df[\"Uz\"].max():.3f}]'), end='')\n",
    "    flags['Uz_absolute_limits'] = 1 if any(abs(df['Uz']) > 10.0) else 0\n",
    "\n",
    "    print('%20s' %(f'[{df[\"Ts\"].min():.2f}, {df[\"Ts\"].max():.2f}]'), end='')\n",
    "    flags['Ts_absolute_limits'] = 1 if any(df['Ts'] > 60.0) or any(df['Ts'] < -50.0) or np.ptp(df['Ts']) > 10.0 else 0\n",
    "\n",
    "    print('%20s' %(f'[{df[\"h2o\"].min():.2f}, {df[\"h2o\"].max():.2f}]'), end='')\n",
    "    flags['h2o_absolute_limits'] = 1 if any(df['h2o'] > 35.0) or any(df['h2o'] < 2.5) or np.ptp(df['h2o']) > 8.0 else 0\n",
    "\n",
    "    print('%20s' %(f'[{df[\"co2\"].min():.2f}, {df[\"co2\"].max():.2f}]'), end='')\n",
    "    flags['co2_absolute_limits'] = 1 if any(df['co2'] > 950.0) or any(df['co2'] < 550.0) or np.ptp(df['co2']) > 120.0 else 0\n",
    "\n",
    "\n",
    "def detrend(df, var):\n",
    "    \"\"\"Linear de-trend\n",
    "    \"\"\"\n",
    "\n",
    "    time_array = df['TmStamp'].values.astype(float) / 1.0E9 # Convert to seconds\n",
    "    time_array -= time_array[0]\n",
    "\n",
    "    s = np.polyfit(time_array, df[var], 1)\n",
    "\n",
    "    trend = s[0] * time_array + s[1]\n",
    "    df[f'{var}_fluct'] = df[var] - trend\n",
    "\n",
    "\n",
    "def higher_moment_statistics(df, var, flags):\n",
    "    \"\"\"Detect possible instrument or recording problems and physical but unusual behavior using higher-moment statistics\n",
    "    The skewness and kurtosis of the fields are computed for the entire record. The record is hard flagged when the\n",
    "    skewness is outside the range (-2, 2) or the kurtosis is outside the range (1, 8).\n",
    "    \"\"\"\n",
    "    m3 = skew(df[f'{var}_fluct'])\n",
    "    m4 = kurtosis(df[f'{var}_fluct'], fisher=False)\n",
    "\n",
    "    print('%20s' %(f'{m3:.3f}, {m4:.3f}'), end='')\n",
    "\n",
    "    if abs(m3) > QC_THRESHOLDS['skewness']:\n",
    "        df[f'{var}_skewness'] = 1\n",
    "\n",
    "    if abs(m4 - 4.5) > QC_THRESHOLDS['kurtosis']:\n",
    "        # The range of kurtosis is (1, 8) in Vickers and Mahrt (1997). It is equivalent to having (kurtosis - 4.5) in\n",
    "        # the range (-3.5, 3.5) in this implementation.\n",
    "        df[f'{var}_kurtosis'] = 1\n",
    "\n",
    "\n",
    "def discontinuities(df, var, flags):\n",
    "    \"\"\"Detect discontinuities in the data using the Haar transform\n",
    "    The transform is computed for a series of moving windows of width L1 and then normalized by the smaller of the\n",
    "    standard deviation for the entire record and one-fourth the range for the entire record. The record is hard flagged\n",
    "    if the absolute value of any single normalized transform exceeds 3 and soft flagged at 2. To identify coherent\n",
    "    changes over the window width L1 in the intensity of the fluctuations, we compute the variance for each half-window\n",
    "    and then compute the difference normalized by the variance over the entire record. The record is hard flagged if the\n",
    "    absolute value of any single normalized transform exceeds 3 and soft flagged at 2.\n",
    "    \"\"\"\n",
    "\n",
    "    L1_SECONDS = 300\n",
    "    L1 = min(int(L1_SECONDS * FREQUENCY_HZ), len(df))\n",
    "\n",
    "\n",
    "    record_std = np.nanstd(df[var])\n",
    "    record_range = np.ptp(df[var])\n",
    "    normal = min(record_std, record_range / 4.0)\n",
    "\n",
    "    window_position = 0\n",
    "\n",
    "    haar_mean_max = 0\n",
    "    haar_variance_max = 0\n",
    "\n",
    "    while (window_position + L1 <= len(df)):\n",
    "        half_point = int(L1 / 2)\n",
    "\n",
    "        half1_mean = np.nanmean(df.loc[0:half_point, var])\n",
    "        half2_mean = np.nanmean(df.loc[half_point:L1, var])\n",
    "\n",
    "        half1_variance = np.var(df.loc[0:half_point, var])\n",
    "        half2_variance = np.var(df.loc[half_point:L1, var])\n",
    "\n",
    "        haar_mean_max = max(haar_mean_max, abs((half2_mean - half1_mean) / normal))\n",
    "        haar_variance_max = max(haar_variance_max, abs((half2_variance - half1_variance) / (record_std * record_std)))\n",
    "\n",
    "        window_position += 1\n",
    "\n",
    "    print('%20s' % (f'{haar_mean_max:.3f}, {haar_variance_max:.3f}'), end='')\n",
    "    flags[f'{var}_discontinuities'] = 1 if (haar_mean_max > QC_THRESHOLDS['discontinuities'] or haar_variance_max > QC_THRESHOLDS['discontinuities']) else 0\n",
    "\n",
    "\n",
    "def nonstationary(df, unit_k, flags):\n",
    "    \"\"\"Identify nonstaionarity of horizontal wind\n",
    "    The wind speed reduction is defined as the ratio of the speed of the vector averaged wind to the averaged\n",
    "    instantaneous speed. When this ratio falls below 0.9, there is some cancellation in the vector average of the wind\n",
    "    components and a soft flag is raised.\n",
    "    The alongwind relative nonstationarity is calculated using linear regression to estimate the difference in the\n",
    "    alongwind component between the beginning and end of the record. This difference normalized by the record mean of\n",
    "    the alongwind component is used to compute the relative nonstationarity.\n",
    "    The crosswind relative nonstationarity is computed from the difference based on the regression of the crosswind\n",
    "    component.\n",
    "    The flow is classified as nonstationary if RNu, RNv, or RNS > 0.50\n",
    "    \"\"\"\n",
    "    # Wind speed reduction\n",
    "    vector_average = math.sqrt(df['Ux'].mean() * df['Ux'].mean() + df['Uy'].mean() * df['Uy'].mean())\n",
    "    instant_average = np.mean(np.sqrt(df['Ux'] * df['Ux'] + df['Uy'] * df['Uy']))\n",
    "    wind_speed_reduction = vector_average / instant_average\n",
    "\n",
    "    print(f'  Wind speed reduction: {wind_speed_reduction:.3f}')\n",
    "    flags['wind_speed_reduction'] = 1 if (wind_speed_reduction < QC_THRESHOLDS['wind_speed_reduction']) else 0\n",
    "\n",
    "    # Relative nonstationarity\n",
    "    ## Calculate alongwind and crosswind components\n",
    "    unit_j = np.cross(unit_k, [df['Ux'].mean(), df['Uy'].mean(), df['Uz'].mean()])\n",
    "    unit_j /= np.linalg.norm(unit_j)\n",
    "    unit_i = np.cross(unit_j, unit_k)\n",
    "\n",
    "    u = df['Ux'] * unit_i[0] + df['Uy'] * unit_i[1] + df['Uz'] * unit_i[2]\n",
    "    v = df['Ux'] * unit_j[0] + df['Uy'] * unit_j[1] + df['Uz'] * unit_j[2]\n",
    "\n",
    "    time_array = df['TmStamp'].values.astype(float) / 1.0E9 # Convert to seconds\n",
    "    time_array -= time_array[0]\n",
    "    su = np.polyfit(time_array, u, 1)\n",
    "    du = su[0] * (time_array[-1] - time_array[0])\n",
    "    sv = np.polyfit(time_array, v, 1)\n",
    "    dv = sv[0] * (time_array[-1] - time_array[0])\n",
    "\n",
    "    rnu = du / np.mean(u)\n",
    "    rnv = dv / np.mean(u)\n",
    "    rns = math.sqrt(du * du + dv * dv) / np.mean(u)\n",
    "\n",
    "    print(f'  Alongwind relative nonstationarity: {rnu:.3f}')\n",
    "    print(f'  Crosswind relative nonstationarity: {rnv:.3f}')\n",
    "    print(f'  Relative nonstationarity: {rns:.3f}')\n",
    "\n",
    "    if (rnu > QC_THRESHOLDS['relative_nonstationarity'] or\n",
    "        rnv > QC_THRESHOLDS['relative_nonstationarity'] or\n",
    "        rns > QC_THRESHOLDS['relative_nonstationarity']):\n",
    "        flags['relative_nonstationarity'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate unit vector k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def unit_vector_k(u, v, w):\n",
    "    \"\"\"Determines unit vector k (parallel to new z-axis)\n",
    "    k: unit vector parallel to new coordinate z axis\n",
    "    b[0]: instrument offset in w1\n",
    "    Adapted from Lee, L., W. Massman, and B. Law, 2004: Handbook of Micrometeorology, Chapt 3, Section 9\n",
    "    \"\"\"\n",
    "    su = np.mean(u)\n",
    "    sv = np.mean(v)\n",
    "    sw = np.mean(w)\n",
    "    suv = np.mean(u * v)\n",
    "    suw = np.mean(u * w)\n",
    "    svw = np.mean(v * w)\n",
    "    su2 = np.mean(u * u)\n",
    "    sv2 = np.mean(v * v)\n",
    "\n",
    "    h = np.array([\n",
    "        [1, su, sv],\n",
    "        [su, su2, suv],\n",
    "        [sv, suv, sv2]\n",
    "    ])\n",
    "    g = np.array([sw, suw, svw])\n",
    "\n",
    "    b = np.linalg.lstsq(h, g, rcond=None)[0]\n",
    "\n",
    "    # Determine unit vector k\n",
    "    unit_k = np.zeros(b.size)\n",
    "    unit_k[2] = 1.0 / (1.0 + b[1] * b[1] + b[2] * b[2])\n",
    "    unit_k[0] = -b[1] * unit_k[2]\n",
    "    unit_k[1] = -b[2] * unit_k[2]\n",
    "    unit_k /= np.linalg.norm(unit_k)\n",
    "\n",
    "    print('\\nUnit vector k of planar fit coordinate:')\n",
    "    print(f'  k_vector = [{unit_k[0]:.3f}, {unit_k[1]:.3f}, {unit_k[2]:.3f}]')\n",
    "    print(f'  b0 = {b[0]:.3f}')\n",
    "\n",
    "    return unit_k, b[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate 30-min flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def calculate_fluxes(unit_k, pressure_kpa, df):\n",
    "    # Transformation to the natural wind coordinate system\n",
    "    # (Lee, L., W. Massman, and B. Law, 2004: Handbook of Micrometeorology, Chapt 4)\n",
    "\n",
    "    u_bar = np.mean(df['Ux'])\n",
    "    v_bar = np.mean(df['Uy'])\n",
    "    w_bar = np.mean(df['Uz'])\n",
    "\n",
    "    # Calculate wind direction and speed\n",
    "    ce = u_bar / math.sqrt(u_bar * u_bar + v_bar * v_bar)\n",
    "    se = v_bar / math.sqrt(u_bar * u_bar + v_bar * v_bar)\n",
    "\n",
    "    eta = math.acos(ce) / math.pi * 180.0\n",
    "    eta = 360.0 - eta if se < 0 else eta\n",
    "    eta = (360.0 - eta + CSAT3_AZIMUTH) % 360.0\n",
    "    print(f'  Wind direction: {eta:.3f} degree')\n",
    "\n",
    "    # Calculate unit vector i and j\n",
    "    unit_j = np.cross(unit_k, [u_bar, v_bar, w_bar])\n",
    "    unit_j /= np.linalg.norm(unit_j)\n",
    "    unit_i = np.cross(unit_j, unit_k)\n",
    "\n",
    "    u = df['Ux_fluct'] * unit_i[0] + df['Uy_fluct'] * unit_i[1] + df['Uz_fluct'] * unit_i[2]\n",
    "    v = df['Ux_fluct'] * unit_j[0] + df['Uy_fluct'] * unit_j[1] + df['Uz_fluct'] * unit_j[2]\n",
    "    w = df['Ux_fluct'] * unit_k[0] + df['Uy_fluct'] * unit_k[1] + df['Uz_fluct'] * unit_k[2]\n",
    "\n",
    "    ust = math.sqrt(math.sqrt(np.mean(u * w) * np.mean(u * w) + np.mean(v * w) * np.mean(v * w)))\n",
    "    print(f'  Friction velocity: {ust:.3f} m/s')\n",
    "\n",
    "    rho = 1.20\n",
    "\n",
    "    f0 = np.mean(w * df['co2_fluct'])\n",
    "    e0 = np.mean(w * df['h2o_fluct']) / 1000.0\n",
    "    sh0 = rho * C_AIR * np.mean(w * df['Ts_fluct'])\n",
    "\n",
    "    if pressure_kpa is not None:\n",
    "        p = pressure_kpa * 1000.0\n",
    "        ta = np.mean(df['Ts']) + 273.15\n",
    "        rho_v = np.mean(df['h2o']) / 1000.0\n",
    "        rho_c = np.mean(df['co2'])\n",
    "\n",
    "        vp = rho_v * RD * ta / 0.622\n",
    "        q = 0.622 * vp / (p - 0.378 * vp)\n",
    "        rho = p / (RD * (1.0 + 0.608 * q) * ta)\n",
    "        rho_d = rho - rho_v\n",
    "\n",
    "        sh = rho * C_AIR * np.mean(w * df['Ts_fluct'])\n",
    "\n",
    "        # Sonic correction below is commented out because it is already done in LI-7500\n",
    "        # H = H + rho * C_AIR * (-0.51 * ta * np.mean(w * df['h2o_fluct'] / 1000.0) / rho\n",
    "\n",
    "        e = (1.0 + 1.6077 * rho_v / rho_d) * (e0 + sh / rho / C_AIR * rho_v / ta)\n",
    "        fc = f0 + 1.6077 * e / rho_d * rho_c / (1.0 + 1.6077 * (rho_v / rho_d)) + sh / rho / C_AIR * rho_c / ta\n",
    "\n",
    "        print(f'  Air pressure = {p:.2f} Pa')\n",
    "        print(f'  Air density = {rho:.2f} kg/m3')\n",
    "        print(f'  Sensible heat flux = {sh:.2f} W/m2')\n",
    "        print(f'  H2O flux before WPL correction = {e0 * LV:.2f} kg/m2')\n",
    "        print(f'  H2O flux after WPL correction = {e * LV:.2f} kg/m2')\n",
    "        print(f'  CO2 flux before WPL correction = {f0 * 1000.0 / 44.0:.2f} umol/m2/s')\n",
    "        print(f'  CO2 flux after WPL correction = {fc * 1000.0 / 44.0:.2f} umol/m2/s')\n",
    "    else:\n",
    "        print(f'  Sensible heat flux: {sh0:.2f} umol/m2/s')\n",
    "        print(f'  Latent heat flux: {e0 * LV:.2f} W/m2')\n",
    "        print(f'  CO2 flux: {f0 * 1000.0 / 44.0:.2f} W/m2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pressure(start, end, df):\n",
    "    \"\"\"Get average air pressure for the averaging time period\n",
    "    \"\"\"\n",
    "    sub_df = df[(df['TmStamp'] >= start) & (df['TmStamp'] < end)]\n",
    "\n",
    "    if len(sub_df) == 0:\n",
    "        print('  No pressure data available')\n",
    "        return None\n",
    "\n",
    "    return sub_df['pressure_irga_mean'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 file:\n",
      "  ../data/test.txt\n",
      "\n",
      "Unit vector k of planar fit coordinate:\n",
      "  k_vector = [-0.141, 0.053, 0.989]\n",
      "  b0 = 0.106\n",
      "\n",
      "2023-09-01 00:30 6000\n",
      "  Available data fraction: 0.333\n",
      "\n",
      "  Diagnostics                                     Ux                  Uy                  Uz                  Ts                 co2                 h2o\n",
      "  Spike fraction                               0.000               0.000               0.001               0.000               0.000               0.000\n",
      "  Empty bin ratio                              0.200               0.210               0.260               0.230               0.210               0.290\n",
      "  Dropouts, extreme dropouts            0.001, 0.000        0.001, 0.000        0.001, 0.001        0.009, 0.009        0.009, 0.009        0.017, 0.017\n",
      "  Range                             [-1.669, -0.567]     [-1.140, 0.103]     [-0.328, 0.382]      [20.00, 21.80]       [9.38, 10.86]    [690.32, 711.72]\n",
      "  Skewness, kurtosis                   -0.097, 2.481       -0.273, 2.797        0.054, 3.357        0.464, 2.700        0.007, 2.460       -0.416, 2.725\n",
      "  Normalized Harr mean, variance        0.577, 0.429        0.043, 0.005        0.041, 0.285        1.345, 0.166        0.773, 0.169        0.699, 0.932\n",
      "\n",
      "  Wind speed reduction: 0.991\n",
      "  Alongwind relative nonstationarity: -0.088\n",
      "  Crosswind relative nonstationarity: -0.062\n",
      "  Relative nonstationarity: 0.108\n",
      "\n",
      "  Flags: instrument\n",
      "\n",
      "  Wind direction: 355.030 degree\n",
      "  Friction velocity: 0.039 m/s\n",
      "  Air pressure = 98031.25 Pa\n",
      "  Air density = 1.16 kg/m3\n",
      "  Sensible heat flux = -3.49 W/m2\n",
      "  H2O flux before WPL correction = 6.74 kg/m2\n",
      "  H2O flux after WPL correction = 6.57 kg/m2\n",
      "  CO2 flux before WPL correction = 0.55 umol/m2/s\n",
      "  CO2 flux after WPL correction = 0.45 umol/m2/s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from setting import AVERAGING_PERIOD_MINUTES\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "files = ['../data/test.txt']\n",
    "start_of_month = datetime(2023, 9, 1)\n",
    "end_of_month = start_of_month + relativedelta(months=1)\n",
    "pres_file = '../data/202309_ten.txt'\n",
    "\n",
    "# Read all files\n",
    "df = pd.DataFrame()\n",
    "print(f'Reading {len(files)} {\"files\" if len(files) > 1 else \"file\"}:')\n",
    "for f in files:\n",
    "    print(f'  {f}')\n",
    "    _df = pd.read_csv(f)\n",
    "    df = pd.concat([df, _df], ignore_index=True)\n",
    "df['TmStamp'] = pd.to_datetime(df['TmStamp'])\n",
    "\n",
    "if pres_file:\n",
    "    WPL = True\n",
    "    pres_df = pd.read_csv(pres_file)\n",
    "    pres_df['TmStamp'] = pd.to_datetime(pres_df['TmStamp'])\n",
    "else:\n",
    "    pres_df = pd.DataFrame()\n",
    "    WPL = False\n",
    "\n",
    "# Data file Diag configuration\n",
    "#\n",
    "# 11 | 10 |  9 |  8 |  7 |  6 |  5 |  4 |  3 |  2 |  1 |  0 |\n",
    "#    CSAT3 flags    |    IRGA flags     |    AGC/6.25       |\n",
    "# CSAT3:\n",
    "# 9: lost trigger special case\n",
    "# 10: no data special case\n",
    "# 11: wrong CSAT3 embedded code special case\n",
    "# 12: SDM error special case\n",
    "# 13: NaN special case\n",
    "#\n",
    "# IRGA:\n",
    "# 1000: chopper\n",
    "# 0100: detector\n",
    "# 0010: pll\n",
    "# 0001: sync\n",
    "\n",
    "# Determine unit vector k of planar fit coordinate\n",
    "# (Lee, L., W. Massman, and B. Law, 2004: Handbook of Micrometeorology, Chapt 3, Section 3)\n",
    "# unit_k is unit vector parallel to new coordinate z axis\n",
    "# bo is instrument offset in w1\n",
    "df = df[df['diag'] < 256]  # Remove bad CSAT3 data\n",
    "unit_k, b0 = unit_vector_k(df['Ux'], df['Uy'], df['Uz'])\n",
    "\n",
    "periods = pd.date_range(start_of_month,end_of_month,freq=f'{AVERAGING_PERIOD_MINUTES}min').to_list()\n",
    "for i in range(len(periods) - 1):\n",
    "    start = periods[i]\n",
    "    end = periods[i + 1]\n",
    "    sub_df = df[(df['TmStamp'] >= start) & (df['TmStamp'] < end) & (df['diag'] < 16)]\n",
    "    if len(sub_df) == 0:\n",
    "        continue\n",
    "\n",
    "    print(f'\\n{end.strftime(\"%Y-%m-%d %H:%M\")} {len(sub_df)}')\n",
    "\n",
    "    flags = {}\n",
    "    quality_control(unit_k, sub_df, flags)\n",
    "\n",
    "    pressure_kpa = get_pressure(start, end, pres_df) if WPL else np.nan\n",
    "    calculate_fluxes(unit_k, pressure_kpa, sub_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
